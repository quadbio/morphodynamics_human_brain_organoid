{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a0dd6c-df1c-476b-9b5e-418006cd06e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import skimage\n",
    "import skimage.io\n",
    "import zarr\n",
    "from joblib import Parallel, delayed\n",
    "from skimage.io import imread, imsave\n",
    "from skimage.transform import rescale\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9c060e-b1f3-4ea8-bf4f-4ac68ad5cd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "channel = \"GFP\"\n",
    "t = 100\n",
    "level = 1\n",
    "\n",
    "\n",
    "def retrieve_tissue_mask(t):\n",
    "    lumen_mask = input_movie[t][\"labels\"][detection_name][level][:]\n",
    "    tissue_mask = input_movie[t][\"labels\"][\"tissue_mask\"][level][:]\n",
    "    tissue_mask = (tissue_mask >= 2).astype(int)\n",
    "    tissue_mask[lumen_mask > 0] = 2\n",
    "    organoid_mask = tissue_mask > 0\n",
    "    return organoid_mask, tissue_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191d0997-b2bd-4958-8dbd-36287e376c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_slice(t, largest_slice, keep_channel_seperate=True):\n",
    "    try:\n",
    "        _, combined_masks = retrieve_tissue_mask(t)\n",
    "    except:\n",
    "        print(\"fail\")\n",
    "        combined_masks = np.zeros(combined_masks.shape).astype(bool)\n",
    "\n",
    "    # GFP channel\n",
    "    image_GFP = input_movie[t][0][largest_slice].astype(np.float32)\n",
    "\n",
    "    # RFP Channel -> load + align\n",
    "\n",
    "    # Transform mask\n",
    "    mask = skimage.transform.resize(\n",
    "        combined_masks[largest_slice], image_GFP.shape, order=0, preserve_range=True\n",
    "    ).astype(\"uint8\")\n",
    "\n",
    "    stack_file = h5py.File(\n",
    "        f\"/cluster/project/treutlein/DATA/imaging/viventis/20220611_200804_ActGFP_LamRFP_TubRFFP_WTC_HB4_D4_pos1-4Mat_9-12_agar_processed/Position_{str(position)}_Settings_1_Processed/denoised_processed.h5\",\n",
    "        \"r\",\n",
    "    )\n",
    "    tissue_mask = input_movie[t][\"labels\"][\"tissue_mask\"][0][:]\n",
    "    non_zero_axis = np.nonzero(tissue_mask)\n",
    "    # tissue_mask=[]\n",
    "    stack_cherry = (\n",
    "        stack_file[f\"t{(t+1):05}\"][\"s00\"][\"0\"][\"cells\"][\n",
    "            (largest_slice - non_zero_axis[0][0])\n",
    "        ]\n",
    "        .astype(np.float32)\n",
    "        .copy()\n",
    "    )\n",
    "    m_cherry_aligned = np.zeros(image_GFP.shape)\n",
    "    m_cherry_aligned[\n",
    "        non_zero_axis[1][0] : non_zero_axis[1][-1] + 1,\n",
    "        non_zero_axis[2][0] : non_zero_axis[2][-1] + 1,\n",
    "    ] = (\n",
    "        rescale(stack_cherry, [0.5, 0.5], anti_aliasing=False, preserve_range=True)\n",
    "        .clip(0.0, 2**16 - 1)\n",
    "        .astype(np.uint16)\n",
    "    )\n",
    "\n",
    "    if keep_channel_seperate:\n",
    "        stack_sep_channel = [\n",
    "            m_cherry_aligned.astype(np.float32),\n",
    "            image_GFP,\n",
    "            (mask == 2).astype(np.float32),\n",
    "        ]\n",
    "    else:\n",
    "        combined_stack = m_cherry_aligned + image_GFP\n",
    "        lumen_mask = input_movie[t][\"labels\"][detection_name][0][:]\n",
    "        organoid_mask = (tissue_mask > 1).astype(int)\n",
    "        organoid_mask[lumen_mask > 1] = 2\n",
    "        stack_sep_channel = [\n",
    "            combined_stack * (mask == 0),\n",
    "            combined_stack * (mask == 1),\n",
    "            combined_stack * (mask == 2),\n",
    "        ]\n",
    "    return stack_sep_channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57d314d-d162-4d39-a479-faa85f2836eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "positions = [3, 12, 13]\n",
    "for position in positions:\n",
    "    detection_name = \"lumen_masks_smooth_5_processed_detection\"\n",
    "    zarr_path = f\"/cluster/project/treutlein/DATA/imaging/viventis/Morphodynamics_of_human_early_brain_organoid_development/tracking/lumen_tracking/Position_{str(position)}_Settings_1_Processed_registered.zarr\"\n",
    "    zarr_array = zarr.open(zarr_path, mode=\"r\")\n",
    "    input_movie = zarr_array[channel]\n",
    "\n",
    "    largest_slice_list = []\n",
    "    for stack in tqdm(range(125)):\n",
    "        try:\n",
    "            organoid_mask, _ = retrieve_tissue_mask(stack)\n",
    "            non_zero_elements = np.count_nonzero(organoid_mask, axis=(1, 2))\n",
    "            percentile_diff = np.abs(\n",
    "                np.percentile(non_zero_elements, 90) - non_zero_elements\n",
    "            )\n",
    "\n",
    "            middle_slice = int(np.where(percentile_diff == percentile_diff.min())[0][0])\n",
    "\n",
    "            largest_slice_list.append(middle_slice)\n",
    "            largest_slice_old = middle_slice\n",
    "        except:\n",
    "            print(\"failed\")\n",
    "            largest_slice_list.append(largest_slice_old)\n",
    "    window = 5\n",
    "    largest_slice_list_mv = np.lib.stride_tricks.sliding_window_view(\n",
    "        np.pad(\n",
    "            largest_slice_list, (0, window - 1), constant_values=largest_slice_list[-1]\n",
    "        ),\n",
    "        window,\n",
    "    ).mean(axis=-1)\n",
    "    largest_slice_list_mv = np.round(largest_slice_list_mv).astype(int)\n",
    "    level = 0\n",
    "    all_stack_sep_channel = Parallel(n_jobs=6, backend=\"multiprocessing\", verbose=5)(\n",
    "        delayed(retrieve_slice)(t, largest_slice)\n",
    "        for t, largest_slice in zip(range(125), largest_slice_list_mv)\n",
    "    )\n",
    "    all_stack_sep_channel = np.array(all_stack_sep_channel)\n",
    "\n",
    "    imsave(\n",
    "        f\"movies/90_percentile_slice/90_percentile_slice_positions_{position}.tiff\",\n",
    "        all_stack_sep_channel,\n",
    "        imagej=True,\n",
    "        resolution=(1.0 / (2 * 0.347), 1.0 / (2 * 0.347)),\n",
    "        metadata={\"unit\": \"um\", \"axes\": \"TCYX\"},\n",
    "        compression=\"zlib\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa2aef1-793b-45f9-9c92-221752b7eb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "for position in range(1, 17):\n",
    "    detection_name = \"lumen_masks_smooth_5_processed_detection\"\n",
    "    zarr_path = f\"/cluster/project/treutlein/DATA/imaging/viventis/Morphodynamics_of_human_early_brain_organoid_development/tracking/lumen_tracking/Position_{str(position)}_Settings_1_Processed_registered.zarr\"\n",
    "    zarr_array = zarr.open(zarr_path, mode=\"r\")\n",
    "    input_movie = zarr_array[channel]\n",
    "\n",
    "    largest_slice_list = []\n",
    "    for stack in tqdm(range(125)):\n",
    "        try:\n",
    "            organoid_mask, _ = retrieve_tissue_mask(stack)\n",
    "            non_zero_elements = np.count_nonzero(organoid_mask, axis=(1, 2))\n",
    "            percentile_diff = np.abs(\n",
    "                np.percentile(non_zero_elements, 90) - non_zero_elements\n",
    "            )\n",
    "\n",
    "            middle_slice = int(np.where(percentile_diff == percentile_diff.min())[0][0])\n",
    "\n",
    "            largest_slice_list.append(middle_slice)\n",
    "            largest_slice_old = middle_slice\n",
    "        except:\n",
    "            print(\"failed\")\n",
    "            largest_slice_list.append(largest_slice_old)\n",
    "    window = 5\n",
    "    largest_slice_list_mv = np.lib.stride_tricks.sliding_window_view(\n",
    "        np.pad(\n",
    "            largest_slice_list, (0, window - 1), constant_values=largest_slice_list[-1]\n",
    "        ),\n",
    "        window,\n",
    "    ).mean(axis=-1)\n",
    "    largest_slice_list_mv = np.round(largest_slice_list_mv).astype(int)\n",
    "    level = 0\n",
    "    all_stack_sep_channel = Parallel(n_jobs=6, backend=\"multiprocessing\", verbose=5)(\n",
    "        delayed(retrieve_slice)(t, largest_slice, keep_channel_seperate=False)\n",
    "        for t, largest_slice in zip(range(125), largest_slice_list_mv)\n",
    "    )\n",
    "    all_stack_sep_channel = np.array(all_stack_sep_channel)\n",
    "\n",
    "    imsave(\n",
    "        f\"movies/90_percentile_slice/90_percentile_slice_positions_{position}_sep_channel.tiff\",\n",
    "        all_stack_sep_channel.astype(np.uint16),\n",
    "        imagej=True,\n",
    "        resolution=(1.0 / (2 * 0.347), 1.0 / (2 * 0.347)),\n",
    "        metadata={\"unit\": \"um\", \"axes\": \"TCYX\"},\n",
    "        compression=\"zlib\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747a09b3-4a6f-4562-b500-58dd571539ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (light_insight)",
   "language": "python",
   "name": "light_insight"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
